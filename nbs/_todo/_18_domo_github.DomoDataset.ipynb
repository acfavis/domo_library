{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: Default description (change me)\n",
    "output-file: domo_github.domodataset.html\n",
    "title: Default Title (change me)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp domo_github.DomoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import asyncio\n",
    "import datetime as dt\n",
    "import importlib\n",
    "import io\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from enum import Enum, auto\n",
    "from pprint import pprint\n",
    "from typing import Any, List\n",
    "\n",
    "import aiohttp\n",
    "import pandas as pd\n",
    "\n",
    "from ..utils import Exceptions as ex\n",
    "from ..utils.Base import Base\n",
    "from ..utils.chunk_execution import chunk_list\n",
    "from ..utils.DictDot import DictDot\n",
    "from . import DomoCertification as dmdc\n",
    "from . import DomoPDP as dmpdp\n",
    "from . import DomoTag as dmtg\n",
    "from .DomoAuth import DomoDeveloperAuth, DomoFullAuth\n",
    "from .routes import dataset_routes\n",
    "\n",
    "importlib.reload(dmtg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class Schema_Column:\n",
    "    name: str\n",
    "    id: str\n",
    "    type: str\n",
    "    dataset: Any\n",
    "\n",
    "    @classmethod\n",
    "    def _from_json(cls, json_obj, domo_dataset):\n",
    "        dd = DictDot(json_obj)\n",
    "        return cls(\n",
    "            name=dd.name,\n",
    "            id=dd.id,\n",
    "            type=dd.type,\n",
    "            dataset=domo_dataset\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass(init=False)\n",
    "class Dataset_Schema:\n",
    "    columns: List[Schema_Column] = field(default_factory=list)\n",
    "\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.columns = []\n",
    "\n",
    "    async def get(self, full_auth: DomoFullAuth = None, debug: bool = False) -> List[Schema_Column]:\n",
    "        full_auth = full_auth or self.dataset.full_auth\n",
    "\n",
    "        res = await dataset_routes.get_schema(full_auth=self.dataset.full_auth, id=self.dataset.id, debug=debug)\n",
    "\n",
    "        if res.status == 200:\n",
    "            json_list = res.response.get('tables')[0].get('columns')\n",
    "\n",
    "            self.columns = []\n",
    "            for json_obj in json_list:\n",
    "                dc = Schema_Column._from_json(\n",
    "                    json_obj=json_obj, domo_dataset=self.dataset)\n",
    "                if dc not in self.columns:\n",
    "                    self.columns.append(dc)\n",
    "\n",
    "            return self.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class DomoDataset:\n",
    "    \"interacts with domo datasets\"\n",
    "    full_auth: DomoFullAuth = field(repr=False, default=None)\n",
    "    dev_auth: DomoDeveloperAuth = field(repr=False, default=None)\n",
    "\n",
    "    id: str = ''\n",
    "    display_type: str = ''\n",
    "    data_provider_type: str = ''\n",
    "    name: str = ''\n",
    "    description: str = ''\n",
    "    row_count: int = None\n",
    "    column_count: int = None\n",
    "    owner: dict = field(default_factory=dict)\n",
    "    formula: dict = field(default_factory=dict)\n",
    "    stream_id: int = None\n",
    "    domo_instance: str = ''\n",
    "\n",
    "    tags: dmtg.Dataset_Tags = None\n",
    "    certification: dmdc.DomoCertification = None\n",
    "    PDPPolicies: dmpdp.Dataset_PDP_Policies = None\n",
    "    schema: Dataset_Schema = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        self.PDPPolicies = dmpdp.Dataset_PDP_Policies(self)\n",
    "        self.schema = Dataset_Schema(self)\n",
    "        self.tags = dmtg.Dataset_Tags(dataset=self)\n",
    "\n",
    "    def display_url(self):\n",
    "        return f'https://{self.domo_instance or self.full_auth.domo_instance}.domo.com/datasources/{self.id}/details/overview'\n",
    "\n",
    "    @classmethod\n",
    "    async def get_from_id(cls,\n",
    "                          id: str,\n",
    "                          full_auth: DomoFullAuth,\n",
    "                          debug: bool = False, log_results: bool = False):\n",
    "\n",
    "        # try:\n",
    "        res = await dataset_routes.get_dataset_by_id(full_auth=full_auth,\n",
    "                                                     id=id or cls.id, debug=debug)\n",
    "        if res.status == 404:\n",
    "            print(\n",
    "                \"f error retrieving get_from_id {full_auth.domo_instance} - {id} status = 404\")\n",
    "            raise ex.InvalidDataset(\n",
    "                domo_instance=full_auth.domo_instance, dataset_id=id)\n",
    "\n",
    "        # except Exception as e:\n",
    "        #     print(e)\n",
    "        #     return None\n",
    "\n",
    "        if debug:\n",
    "            pprint(res)\n",
    "\n",
    "        dd = DictDot(res.response)\n",
    "        ds = cls(\n",
    "            full_auth=full_auth,\n",
    "            id=dd.id,\n",
    "            display_type=dd.displayType,\n",
    "            data_provider_type=dd.dataProviderType,\n",
    "            name=dd.name,\n",
    "            description=dd.description,\n",
    "            owner=dd.owner,\n",
    "            formula=dd.properties.formulas.formulas,\n",
    "            stream_id=dd.streamId,\n",
    "            row_count=int(dd.rowCount),\n",
    "            column_count=int(dd.columnCount)\n",
    "        )\n",
    "\n",
    "        if dd.tags:\n",
    "            ds.tags.tag_ls = json.loads(dd.tags)\n",
    "\n",
    "        if dd.certification:\n",
    "            # print('class def certification', dd.certification)\n",
    "            ds.certification = dmdc.DomoCertification._from_json(\n",
    "                dd.certification)\n",
    "\n",
    "        return ds\n",
    "\n",
    "    @classmethod\n",
    "    async def query_dataset(cls,\n",
    "                            sql: str,\n",
    "                            dataset_id: str,\n",
    "                            dev_auth: DomoDeveloperAuth,\n",
    "                            debug: bool = False,\n",
    "                            session: aiohttp.ClientSession = None) -> pd.DataFrame:\n",
    "\n",
    "        if debug:\n",
    "            print(\"query dataset class method\")\n",
    "            print({'dataset_id': dataset_id,\n",
    "                   'dev_auth': dev_auth})\n",
    "\n",
    "        res = await dataset_routes.query_dataset_public(dev_auth=dev_auth, id=dataset_id, sql=sql, session=session,\n",
    "                                                        debug=debug)\n",
    "\n",
    "        if debug:\n",
    "            print(res.response)\n",
    "\n",
    "        if res.status == 200:\n",
    "            df = pd.DataFrame(data=res.response.get('rows'),\n",
    "                              columns=res.response.get('columns'))\n",
    "            return df\n",
    "        return None\n",
    "\n",
    "    @classmethod\n",
    "    async def query_dataset_private(cls,\n",
    "                                    sql: str,\n",
    "                                    dataset_id: str,\n",
    "                                    full_auth: DomoFullAuth,\n",
    "                                    debug: bool = False,\n",
    "                                    session: aiohttp.ClientSession = None) -> pd.DataFrame:\n",
    "\n",
    "        if debug:\n",
    "            print(\"query dataset class method\")\n",
    "            print({'dataset_id': dataset_id,\n",
    "                   'full_auth': full_auth})\n",
    "\n",
    "        res = await dataset_routes.query_dataset_private(full_auth=full_auth, id=dataset_id, sql=sql, session=session,\n",
    "                                                         debug=debug)\n",
    "\n",
    "        return pd.DataFrame(res)\n",
    "\n",
    "    async def upload_csv(self,\n",
    "                         upload_df: pd.DataFrame = None,\n",
    "                         upload_df_list: list[pd.DataFrame] = None,\n",
    "                         upload_file: io.TextIOWrapper = None,\n",
    "\n",
    "                         full_auth: DomoFullAuth = None,\n",
    "                         upload_method: str = 'REPLACE',\n",
    "                         dataset_id: str = None,\n",
    "                         dataset_upload_id=None,\n",
    "                         partition_key: str = None,\n",
    "                         is_index: bool = True,\n",
    "                         session: aiohttp.ClientSession = None,\n",
    "                         debug: bool = False):\n",
    "\n",
    "        full_auth = full_auth or self.full_auth\n",
    "        dataset_id = dataset_id or self.id\n",
    "\n",
    "        upload_df_list = upload_df_list or [upload_df]\n",
    "\n",
    "        # stage 1 get uploadId\n",
    "        if not dataset_upload_id:\n",
    "            if debug:\n",
    "                print(f\"\\n\\nðŸŽ­ starting Stage 1\")\n",
    "\n",
    "            res = await dataset_routes.upload_dataset_stage_1(full_auth=full_auth,\n",
    "                                                              dataset_id=dataset_id,\n",
    "                                                              session=session,\n",
    "                                                              data_tag=partition_key,\n",
    "                                                              debug=debug\n",
    "                                                              )\n",
    "            if debug:\n",
    "                print(f\"\\n\\nðŸŽ­ Stage 1 response -- {res.status}\")\n",
    "                print(res)\n",
    "\n",
    "            dataset_upload_id = res.response.get('uploadId')\n",
    "\n",
    "        # stage 2 upload_dataset\n",
    "\n",
    "        if debug:\n",
    "            print(\n",
    "                f\"\\n\\nðŸŽ­ starting Stage 2 - {len(upload_df_list)} - number of parts\")\n",
    "\n",
    "        stage_2_res = None\n",
    "\n",
    "        if upload_file:\n",
    "            if debug:\n",
    "                print('stage 2 - file')\n",
    "            stage_2_res = await dataset_routes.upload_dataset_stage_2_file(full_auth=full_auth,\n",
    "                                                                           dataset_id=dataset_id,\n",
    "                                                                           upload_id=dataset_upload_id,\n",
    "                                                                           part_id=1,\n",
    "                                                                           file=upload_file,\n",
    "                                                                           session=session, debug=debug)\n",
    "            if debug:\n",
    "                print(f\"ðŸŽ­ Stage 2 response -- {stage_2_res.status}\")\n",
    "                print(stage_2_res.print(is_pretty=True))\n",
    "\n",
    "        else:\n",
    "            if debug:\n",
    "                print('stage 2 - df')\n",
    "            stage_2_res = await asyncio.gather(*[dataset_routes.upload_dataset_stage_2_df(full_auth=full_auth,\n",
    "                                                                                          dataset_id=dataset_id,\n",
    "                                                                                          upload_id=dataset_upload_id,\n",
    "                                                                                          part_id=index + 1,\n",
    "                                                                                          upload_df=df,\n",
    "                                                                                          session=session, debug=debug) for index, df in enumerate(upload_df_list)])\n",
    "\n",
    "            if debug:\n",
    "                for res in stage_2_res:\n",
    "                    print(f\"ðŸŽ­ Stage 2 response -- {res.status}\")\n",
    "                    res.print(is_pretty=True)\n",
    "\n",
    "        # return stage_2_res\n",
    "\n",
    "#         # stage 3 commit_data\n",
    "        if debug:\n",
    "            print(f\"\\n\\nðŸŽ­ starting Stage 3\")\n",
    "        await asyncio.sleep(10)\n",
    "\n",
    "        stage3_res = await dataset_routes.upload_dataset_stage_3(full_auth=full_auth,\n",
    "                                                                 dataset_id=dataset_id,\n",
    "                                                                 upload_id=dataset_upload_id,\n",
    "                                                                 update_method=upload_method,\n",
    "                                                                 data_tag=partition_key,\n",
    "                                                                 is_index=False,\n",
    "                                                                 session=session,\n",
    "                                                                 debug=debug)\n",
    "\n",
    "        if debug:\n",
    "            print(f\"\\nðŸŽ­ stage 3 res - {res.status}\")\n",
    "            print(stage3_res)\n",
    "\n",
    "        if is_index:\n",
    "            await self.index_dataset(full_auth=full_auth,\n",
    "                                     dataset_id=dataset_id,\n",
    "                                     debug=debug,\n",
    "                                     session=session)\n",
    "\n",
    "        return stage3_res\n",
    "\n",
    "    async def index_dataset(self,\n",
    "                            full_auth: DomoFullAuth = None,\n",
    "                            dataset_id: str = None,\n",
    "                            debug: bool = False,\n",
    "                            session: aiohttp.ClientSession = None\n",
    "                            ):\n",
    "\n",
    "        full_auth = full_auth or self.full_auth\n",
    "        dataset_id = dataset_id or self.id\n",
    "        return await dataset_routes.index_dataset(full_auth=full_auth, dataset_id=dataset_id, debug=debug,\n",
    "                                                  session=session)\n",
    "\n",
    "    async def list_partitions(self,\n",
    "                              full_auth: DomoFullAuth = None,\n",
    "                              dataset_id: str = None,\n",
    "                              debug: bool = False,\n",
    "                              session: aiohttp.ClientSession = None\n",
    "                              ):\n",
    "\n",
    "        full_auth = full_auth or self.full_auth\n",
    "        dataset_id = dataset_id or self.id\n",
    "\n",
    "        res = await dataset_routes.list_partitions(full_auth=full_auth, dataset_id=dataset_id, debug=debug,\n",
    "                                                   session=session)\n",
    "        if res.status != 200:\n",
    "            return None\n",
    "        return res.response\n",
    "\n",
    "    async def delete_partition(self,\n",
    "                               dataset_partition_id: str,\n",
    "\n",
    "                               dataset_id: str = None,\n",
    "                               empty_df: pd.DataFrame = None,\n",
    "\n",
    "                               full_auth: DomoFullAuth = None,\n",
    "\n",
    "                               is_index: bool = True,\n",
    "                               debug: bool = False,\n",
    "                               session: aiohttp.ClientSession = None):\n",
    "\n",
    "        is_close_session = True if not session else False\n",
    "\n",
    "        session = session or aiohttp.ClientSession()\n",
    "        full_auth = full_auth or self.full_auth\n",
    "        dataset_id = dataset_id or self.id\n",
    "\n",
    "#        if empty_df is None:\n",
    "#            empty_df = await self.query_dataset_private(full_auth=full_auth,\n",
    "#                                                        dataset_id=dataset_id,\n",
    "#                                                        sql=\"SELECT * from table limit 1\",\n",
    "#                                                        debug=False)\n",
    "#\n",
    "#        await self.upload_csv(upload_df=empty_df.head(0),\n",
    "#                              upload_method='REPLACE',\n",
    "#                              is_index=is_index,\n",
    "#                              partition_key=dataset_partition_id,\n",
    "#                              session=session,\n",
    "#                              debug=False)\n",
    "        if debug:\n",
    "            print(f\"\\n\\nðŸŽ­ starting Stage 1\")\n",
    "\n",
    "        res = await dataset_routes.delete_partition_stage_1(full_auth=full_auth,\n",
    "                                                            dataset_id=dataset_id,\n",
    "                                                            dataset_partition_id=dataset_partition_id,\n",
    "                                                            debug=debug, session=session)\n",
    "        if debug:\n",
    "            print(f\"\\n\\nðŸŽ­ Stage 1 response -- {res.status}\")\n",
    "            print(res)\n",
    "\n",
    "        stage_2_res = None\n",
    "        if debug:\n",
    "            print('starting Stage 2')\n",
    "        stage_2_res = await dataset_routes.delete_partition_stage_2(full_auth=full_auth,\n",
    "                                                                    dataset_id=dataset_id,\n",
    "                                                                    dataset_partition_id=dataset_partition_id,\n",
    "                                                                    debug=debug, session=session)\n",
    "        if debug:\n",
    "            print(f\"\\n\\nðŸŽ­ Stage 2 response -- {stage_2_res.status}\")\n",
    "\n",
    "        stage_3_res = None\n",
    "        if debug:\n",
    "            print('starting Stage 3')\n",
    "        stage_3_res = await dataset_routes.index_dataset(full_auth=full_auth,\n",
    "                                                         dataset_id=dataset_id,\n",
    "                                                         debug=debug, session=session)\n",
    "        if debug:\n",
    "            print(f\"\\n\\nðŸŽ­ Stage 3 response -- {stage_3_res.status}\")\n",
    "\n",
    "        if is_close_session:\n",
    "            await session.close()\n",
    "\n",
    "        if debug:\n",
    "            print(stage_3_res)\n",
    "\n",
    "        if stage_3_res.status == 200:\n",
    "            return res.response\n",
    "\n",
    "    async def reset_dataset(self,\n",
    "                            full_auth: DomoFullAuth = None,\n",
    "                            is_index: bool = True,\n",
    "                            debug: bool = False\n",
    "                            ):\n",
    "        execute_reset = input(\n",
    "            \"This function will delete all rows.  Type BLOW_ME_AWAY to execute:\")\n",
    "\n",
    "        if execute_reset != 'BLOW_ME_AWAY':\n",
    "            print(\"You didn't type BLOW_ME_AWAY, moving on.\")\n",
    "            return None\n",
    "\n",
    "        full_auth = full_auth or self.full_auth\n",
    "        dataset_id = self.id\n",
    "\n",
    "        if not full_auth:\n",
    "            raise Exception(\"full_auth required\")\n",
    "\n",
    "        session = aiohttp.ClientSession()\n",
    "\n",
    "        # create empty dataset to retain schema\n",
    "        empty_df = await self.query_dataset_private(full_auth=full_auth,\n",
    "                                                    dataset_id=dataset_id,\n",
    "                                                    sql=\"SELECT * from table limit 1\",\n",
    "                                                    session=session,\n",
    "                                                    debug=debug)\n",
    "        empty_df = empty_df.head(0)\n",
    "\n",
    "        # get partition list\n",
    "#         partition_list = await dataset_routes.list_partitions(full_auth=full_auth,\n",
    "#                                                               dataset_id=self.id,\n",
    "#                                                               debug=debug,\n",
    "#                                                               session=session)\n",
    "\n",
    "#         if len(partition_list) > 0:\n",
    "#             partition_list = chunk_list(partition_list, 100)\n",
    "\n",
    "#             for index, pl in enumerate(partition_list):\n",
    "#                 print(f'ðŸ¥« starting chunk {index + 1} of {len(partition_list)}')\n",
    "\n",
    "#                 await asyncio.gather(*[self.delete_partition(full_auth=full_auth,\n",
    "#                                                              dataset_partition_id=partition.get('partitionId'),\n",
    "#                                                              session=session,\n",
    "#                                                              empty_df=empty_df,\n",
    "#                                                              debug=False) for partition in pl])\n",
    "#                 if is_index:\n",
    "#                     await self.index_dataset(session=session)\n",
    "\n",
    "        res = await self.upload_csv(upload_df=empty_df,\n",
    "                                    upload_method='REPLACE',\n",
    "                                    is_index=is_index,\n",
    "                                    session=session,\n",
    "                                    debug=False)\n",
    "\n",
    "        await session.close()\n",
    "        return True\n",
    "\n",
    "    async def delete(self,\n",
    "                     dataset_id=None,\n",
    "                     full_auth: DomoFullAuth = None,\n",
    "                     debug: bool = False,\n",
    "                     session: aiohttp.ClientSession = None):\n",
    "        try:\n",
    "            is_close_session = False\n",
    "\n",
    "            if not session:\n",
    "                session = aiohttp.ClientSession()\n",
    "                is_close_session = True\n",
    "\n",
    "            return await dataset_routes.delete(\n",
    "                full_auth=full_auth or self.full_auth,\n",
    "                dataset_id=dataset_id or self.id,\n",
    "                debug=debug,\n",
    "                session=session)\n",
    "\n",
    "        finally:\n",
    "            if is_close_session:\n",
    "                await session.close()\n",
    "\n",
    "    # async def create(self,\n",
    "    #                   ds_name,\n",
    "    #                   ds_type ='api',\n",
    "    #                   schema = { \"columns\": [ {\n",
    "    #                       \"name\": 'col1',\n",
    "    #                       \"type\": 'LONG',\n",
    "    #                       \"metadata\": None,\n",
    "    #                       \"upsertKey\": False}\n",
    "    #                   ]},\n",
    "    #                   full_auth:DomoFullAuth = None,\n",
    "    #                   debug:bool = False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
