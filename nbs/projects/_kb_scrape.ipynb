{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sudo apt-get update\n",
    "# sudo apt install chromium-chromedriver\n",
    "# sudo apt install chromium-bsu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dataclasses import dataclass\n",
    "# import pandas as pd\n",
    "# from sqlalchemy.util import dataclass_fields\n",
    "\n",
    "# from urllib.parse import urljoin\n",
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import re\n",
    "# from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import sqlite3\n",
    "from fastcore.basics import patch_to\n",
    "\n",
    "# download_url\n",
    "import time\n",
    "import selenium\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "\n",
    "    sqlite_driver_db: str\n",
    "    sqlit_con: sqlite3.connect\n",
    "\n",
    "    url_to_visit_ls: list[str]\n",
    "\n",
    "    url_visited_ls: list[str] = []\n",
    "    article_ls : list[str] = []\n",
    "\n",
    "    def __init__(self,\n",
    "                 sqlite_driver_db: str,\n",
    "                 url_to_visit_ls: list[str] = [],\n",
    "                 base_url: str = None):\n",
    "\n",
    "        self.sqlite_driver_db = sqlite_driver_db\n",
    "        self.sqlit_con = sqlite3.connect(self.sqlite_driver_db)\n",
    "        self.base_url = base_url\n",
    "        self.url_to_visit_ls = url_to_visit_ls\n",
    "\n",
    "\n",
    "    \n",
    "    def _add_url_to_visit(self, url ):\n",
    "      if url not in self.url_visited_ls and url not in self.url_to_visit_ls:\n",
    "        self.url_to_visit_ls.append(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    level=logging.INFO)\n",
    "\n",
    "cd = Crawler(sqlite_driver_db = 'kb_scrape', base_url = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(Crawler)\n",
    "def run(self: Crawler, debug_prn: bool = False):\n",
    "    \n",
    "    counter = 0\n",
    "    upload_counter = 0\n",
    "    num_upload = 20\n",
    "    offset = 0\n",
    "    max_upload = 10000\n",
    "\n",
    "    while self.url_to_visit_ls and counter <= max_upload:\n",
    "        url = self.url_to_visit_ls.pop(0)\n",
    "        \n",
    "        logging.info(f'Crawling: {url}')\n",
    "        \n",
    "        try:\n",
    "            # self.crawl(url, debug_prn)\n",
    "\n",
    "            pass\n",
    "            \n",
    "        except Exception:\n",
    "            logging.exception(f'Failed to crawl: {url}')\n",
    "\n",
    "        finally:\n",
    "            self.url_visited_ls.append(url)\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "    #     if counter == 10:\n",
    "    #     self.update_exist_ls()\n",
    "\n",
    "    #     if counter % num_upload == 0:\n",
    "    #     offset = num_upload * upload_counter\n",
    "    #     df = pd.DataFrame(self.article_ls).iloc[offset:]\n",
    "\n",
    "    #     print(f\"rows uploading {offset} - {counter}\")\n",
    "\n",
    "    #     if upload_counter == 0:\n",
    "    #         self.insert_into_sql(df, is_reset=True)\n",
    "    #     else:\n",
    "    #         self.insert_into_sql(df, is_reset=False)\n",
    "\n",
    "    #     upload_counter += 1\n",
    "\n",
    "    # df = pd.DataFrame(self.article_ls).iloc[offset:]\n",
    "\n",
    "    # self.insert_into_sql(df, is_reset=False)\n",
    "\n",
    "    print('RUN: done crawling')\n",
    "\n",
    "    return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-25 02:42:51,076 INFO:Crawling: https://domo-support.domo.com/s/article/360047400753?language=en_US\n",
      "2023-01-25 02:42:51,077 INFO:Crawling: https://domo-support.domo.com/s/topic/0TO5w000000ZaoEGAS/sharing-access-to-cards-and-pages?language=en_US\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUN: done crawling\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Crawler>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)s:%(message)s',\n",
    "    level=logging.INFO)\n",
    "\n",
    "url_to_visit_ls = [\n",
    "    'https://domo-support.domo.com/s/article/360047400753?language=en_US', 'https://domo-support.domo.com/s/topic/0TO5w000000ZaoEGAS/sharing-access-to-cards-and-pages?language=en_US']\n",
    "\n",
    "cd = Crawler(sqlite_driver_db='kb_scrape', base_url='', url_to_visit_ls= url_to_visit_ls)\n",
    "\n",
    "cd.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(Crawler)\n",
    "def download_url(self: Crawler, url, debug_prn: bool = False):\n",
    "\n",
    "  options = webdriver.ChromeOptions()\n",
    "  options.add_argument(\"-headless\")\n",
    "  options.add_argument(\"-no-sandbox\")\n",
    "  options.add_argument(\"-disable-dev-shm-usage\")\n",
    "\n",
    "  wd = webdriver.Chrome(\"chromedriver\", options=options)\n",
    "\n",
    "  if debug_prn:\n",
    "        print(f'downloading data from {url}')\n",
    "\n",
    "  wd.get(url)\n",
    "  time.sleep(5)  # sleeps for 5 seconds to give page time to load\n",
    "  source = wd.page_source\n",
    "\n",
    "  return source\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: Service chromedriver unexpectedly exited. Status code was: 1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m options\u001b[39m.\u001b[39madd_argument(\u001b[39m\"\u001b[39m\u001b[39m-no-sandbox\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m options\u001b[39m.\u001b[39madd_argument(\u001b[39m\"\u001b[39m\u001b[39m-disable-dev-shm-usage\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m wd \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39;49mChrome(\u001b[39m\"\u001b[39;49m\u001b[39mchromedriver\u001b[39;49m\u001b[39m\"\u001b[39;49m, options\u001b[39m=\u001b[39;49moptions)\n\u001b[1;32m     12\u001b[0m wd\u001b[39m.\u001b[39mget(url)\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/selenium/webdriver/chrome/webdriver.py:80\u001b[0m, in \u001b[0;36mWebDriver.__init__\u001b[0;34m(self, executable_path, port, options, service_args, desired_capabilities, service_log_path, chrome_options, service, keep_alive)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m service:\n\u001b[1;32m     78\u001b[0m     service \u001b[39m=\u001b[39m Service(executable_path, port, service_args, service_log_path)\n\u001b[0;32m---> 80\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\n\u001b[1;32m     81\u001b[0m     DesiredCapabilities\u001b[39m.\u001b[39;49mCHROME[\u001b[39m\"\u001b[39;49m\u001b[39mbrowserName\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     82\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mgoog\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     83\u001b[0m     port,\n\u001b[1;32m     84\u001b[0m     options,\n\u001b[1;32m     85\u001b[0m     service_args,\n\u001b[1;32m     86\u001b[0m     desired_capabilities,\n\u001b[1;32m     87\u001b[0m     service_log_path,\n\u001b[1;32m     88\u001b[0m     service,\n\u001b[1;32m     89\u001b[0m     keep_alive,\n\u001b[1;32m     90\u001b[0m )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/selenium/webdriver/chromium/webdriver.py:101\u001b[0m, in \u001b[0;36mChromiumDriver.__init__\u001b[0;34m(self, browser_name, vendor_prefix, port, options, service_args, desired_capabilities, service_log_path, service, keep_alive)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mservice cannot be None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mservice \u001b[39m=\u001b[39m service\n\u001b[0;32m--> 101\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mservice\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m    103\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    104\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\n\u001b[1;32m    105\u001b[0m         command_executor\u001b[39m=\u001b[39mChromiumRemoteConnection(\n\u001b[1;32m    106\u001b[0m             remote_server_addr\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mservice\u001b[39m.\u001b[39mservice_url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m         options\u001b[39m=\u001b[39moptions,\n\u001b[1;32m    113\u001b[0m     )\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/selenium/webdriver/common/service.py:104\u001b[0m, in \u001b[0;36mService.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m count \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    103\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 104\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49massert_process_still_running()\n\u001b[1;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_connectable():\n\u001b[1;32m    106\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.python/current/lib/python3.10/site-packages/selenium/webdriver/common/service.py:117\u001b[0m, in \u001b[0;36mService.assert_process_still_running\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    115\u001b[0m return_code \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess\u001b[39m.\u001b[39mpoll()\n\u001b[1;32m    116\u001b[0m \u001b[39mif\u001b[39;00m return_code:\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mraise\u001b[39;00m WebDriverException(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mService \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath\u001b[39m}\u001b[39;00m\u001b[39m unexpectedly exited. Status code was: \u001b[39m\u001b[39m{\u001b[39;00mreturn_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: Service chromedriver unexpectedly exited. Status code was: 1\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"-headless\")\n",
    "options.add_argument(\"-no-sandbox\")\n",
    "options.add_argument(\"-disable-dev-shm-usage\")\n",
    "\n",
    "wd = webdriver.Chrome(\"chromedriver\", options=options)\n",
    "\n",
    "wd.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch_to(Crawler)\n",
    "def crawl(self : Crawler, url, debug_prn: bool = False):\n",
    "    \n",
    "    if debug_prn:\n",
    "        print(f\"CRAWL:  starting crawl - {url}\")\n",
    "\n",
    "    html = self.download_url(url, debug_prn=debug_prn)\n",
    "\n",
    "    # article = self.get_article(html, url)\n",
    "\n",
    "    linked_urls_ls = None\n",
    "    # linked_urls_ls = article.get('linked_urls_ls')\n",
    "\n",
    "    if not linked_urls_ls or len(linked_urls_ls) == 0:\n",
    "        return\n",
    "\n",
    "    for url in linked_urls_ls:\n",
    "        self._add_url_to_visit(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def update_exist_ls(self):\n",
    "#       \"\"\"updates crawler's visited_urls based on my database of existing kbs\"\"\"\n",
    "#       cursor = self.sqlit_con.execute('''SELECT url FROM kb''')\n",
    "\n",
    "#       for row in cursor:\n",
    "#         self.visited_urls.append(row[0])\n",
    "\n",
    "\n",
    "\n",
    "#     def get_linked_urls(self, html, debug_prn: bool = False):\n",
    "#       import urllib.parse as url_parse\n",
    "#       from bs4 import BeautifulSoup\n",
    "\n",
    "#       soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#       linked_urls = []\n",
    "#       for link in soup.find_all('a'):\n",
    "#           path = link.get('href')\n",
    "\n",
    "#           if not path:\n",
    "#             continue\n",
    "\n",
    "#           if path.startswith('/'):\n",
    "#               path = url_parse.urljoin(self.base_url, path)\n",
    "\n",
    "#           if path.startswith(self.base_url):\n",
    "#             linked_urls.append(path)\n",
    "\n",
    "#       return linked_urls\n",
    "\n",
    "\n",
    "\n",
    "#     def insert_into_sql(self, article_df, is_reset: bool = False):\n",
    "#       con = sqlite3.connect(\n",
    "#           \"/content/drive/MyDrive/Colab Notebooks/webscrape_domo/my_db2.db\")\n",
    "\n",
    "#       print(article_df.__dict__)\n",
    "\n",
    "#       article_df.linked_urls = \", \".join(article_df.linked_urls)\n",
    "\n",
    "#       # display(HTML(article_df.to_html()))\n",
    "\n",
    "#       if is_reset:\n",
    "#         article_df.to_sql(\"kb\", con, if_exists=\"replace\")\n",
    "\n",
    "#       article_df.to_sql(\"kb\", con, if_exists=\"append\")\n",
    "\n",
    "#     def get_article(self, html, url):\n",
    "#       soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#       form_labels = soup.find_all(class_=['slds-form-element'])\n",
    "\n",
    "#       linked_urls_ls = self.get_linked_urls(html)\n",
    "\n",
    "#       article = {\"url\": url,\n",
    "#                  \"linked_urls\": linked_urls_ls}\n",
    "\n",
    "#       for form in form_labels:\n",
    "#         rows = list(form.strings)\n",
    "\n",
    "#         if len(rows) >= 2:\n",
    "#           title = rows[0]\n",
    "#           value_ls = rows[1:]\n",
    "\n",
    "#           value_tx = \" \".join(value_ls)\n",
    "\n",
    "#           value_clean = re.sub(r'( \\n.?)+', r'\\r', value_tx)\n",
    "#           article.update({title: value_clean})\n",
    "\n",
    "#       if article.get('Article Body'):\n",
    "#         self.article_ls.append(article)\n",
    "#       else:\n",
    "#         self.add_url_to_visit(url)\n",
    "\n",
    "#       return article\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Linux-5.4.0-1100-azure-x86_64-with-glibc2.31'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import platform\n",
    "platform.platform()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
